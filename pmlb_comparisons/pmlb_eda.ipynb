{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pmlb as dsets\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from os.path import join as oj\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from numpy import array as arr\n",
    "\n",
    "# sklearn models\n",
    "sys.path.append('../scores')\n",
    "import scores\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "import interactions\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/scratch/users/vision/data/pmlb'\n",
    "out_dir = '/scratch/users/vision/chandan/pmlb'\n",
    "dset_names = deepcopy(dsets.classification_dataset_names)\n",
    "dset_names.remove('kddcup') # # remove biggest dset\n",
    "dset_names.remove('mnist') # # remove biggest dset\n",
    "dset_names.remove('poker') # # remove biggest dset\n",
    "\n",
    "dset_names = np.array(dset_names)\n",
    "random_state = 42 # for each train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train models\n",
    "this code will save all the **classification** dsets and fit very quick models to each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logit_and_rfs():\n",
    "    logit_test_scores = []\n",
    "    rf_test_scores = []\n",
    "    rfs = []\n",
    "\n",
    "    for dset_name in tqdm(dset_names):\n",
    "        X, y = dsets.fetch_data(dset_name, return_X_y=True, \n",
    "                          local_cache_dir=data_dir)\n",
    "\n",
    "\n",
    "        train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=random_state)\n",
    "\n",
    "        logit = LogisticRegression(solver='liblinear', multi_class='auto') # liblinear best for small dsets, otherwise lbfgs\n",
    "        rf = RandomForestClassifier(n_estimators=100)\n",
    "    #     print(dset_name, X.shape)\n",
    "        logit.fit(train_X, train_y)\n",
    "        rf.fit(train_X, train_y)\n",
    "\n",
    "        logit_test_scores.append(logit.score(test_X, test_y))\n",
    "        rf_test_scores.append(rf.score(test_X, test_y))\n",
    "        rfs.append(deepcopy(rf))\n",
    "\n",
    "    # save\n",
    "    logit_test_scores = np.array(logit_test_scores)\n",
    "    rf_test_scores = np.array(rf_test_scores)\n",
    "    classification_results = {'logit_test_score': logit_test_scores,\n",
    "               'rf_test_score': rf_test_scores,\n",
    "               'dset_name': dset_names,\n",
    "               'rf': rfs}\n",
    "    pkl.dump(classification_results, \n",
    "             open(oj(out_dir, 'classification_results.pkl'), 'wb'))\n",
    "# fit_logit_and_rfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(pkl.load(open(oj(out_dir, 'classification_results.pkl'), 'rb')))\n",
    "\n",
    "# plot\n",
    "# sns.boxplot(data=[results['logit_test_score'], results['rf_test_score']])\n",
    "# plt.xticks([0, 1], ['Logistic Regression', 'Random Forest'])\n",
    "# plt.ylabel('Test Accuracy')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# singlevar logistic vs logistic of curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right now assumes feat_nums is of length 1\n",
    "def get_feats(X, feat_nums=[0]):\n",
    "    if len(feat_nums) == 1:\n",
    "        return X[:, feat_nums[0]].reshape(-1, 1)\n",
    "    else:\n",
    "        return X[:, feat_num]\n",
    "\n",
    "def score_logistic_onevar(train_X, train_y, test_X, test_y, feat_nums, f):\n",
    "    logit = LogisticRegression(solver='liblinear', multi_class='auto') # liblinear best for small dsets, otherwise lbfgs\n",
    "\n",
    "    # full training\n",
    "#     print('full logit score', row.logit_test_score)\n",
    "\n",
    "    logit.fit(get_feats(train_X, feat_nums), train_y)\n",
    "    logit_score_orig_onevar = logit.score(get_feats(test_X, feat_nums), test_y)\n",
    "\n",
    "    logit.fit(f(get_feats(train_X, feat_nums)), train_y)\n",
    "    logit_score_altered_onevar = logit.score(get_feats(f(test_X), feat_nums), test_y)\n",
    "    \n",
    "    return logit_score_orig_onevar, logit_score_altered_onevar    \n",
    "    \n",
    "'''\n",
    "arg1 - trained forest\n",
    "arg2 - X\n",
    "arg3 - y\n",
    "arg4 - S (array of size num_features, 0 to not use this variable otherwise 1)\n",
    "\n",
    "returns: value of function on a line at regular intervals\n",
    "'''\n",
    "def single_var_grid_scores_and_plot(forest, X, y, S, curve_range=None, step=None, plot=True):\n",
    "    # deal with params\n",
    "    if curve_range is None:\n",
    "        curve_range = (np.min(X), np.max(X))\n",
    "    step = (curve_range[1] - curve_range[0]) / 100\n",
    "    curve_range = (curve_range[0], curve_range[1] + 10 * step) # do this so we can interpolate properly\n",
    "    x_axis = np.arange(curve_range[0], curve_range[1], step)\n",
    "    \n",
    "    models = forest.estimators_\n",
    "    length = (curve_range[1] - curve_range[0]) / (1.0 * step)\n",
    "    line = np.zeros(x_axis.shape[0])\n",
    "    index = np.nonzero(S)[0][0]\n",
    "    num_vars = len(S)\n",
    "    for model in models:\n",
    "        vals = interactions.traverse_all_paths(model, X, y, S, continuous_y=True)\n",
    "        line += interactions.make_line(vals, x_axis, step, S)\n",
    "    line = line / (len(models) * 1.0)\n",
    "    \n",
    "    \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(8, 6, forward=True)\n",
    "        plt.plot(x_axis, line, 'k', c='b')\n",
    "        plt.show()\n",
    "    return x_axis, line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/163 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163, 4) (163, 4)\n",
      "num idxs after filtering 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 61/163 [57:06<1:29:58, 52.92s/it]"
     ]
    }
   ],
   "source": [
    "# get dsets where rf outperforms logistic\n",
    "idxs_mask = results['rf_test_score'] - results['logit_test_score'] > 0.1 \n",
    "# r = results[idxs_mask]\n",
    "r = results\n",
    "print(results.shape, r.shape)\n",
    "print('num idxs after filtering', np.sum(idxs_mask))\n",
    "# idxs = np.arange(idxs_mask.size)[idxs_mask] # get actual indexes for this mask\n",
    "\n",
    "score_results = {\n",
    "        'feature_scores_mdi': [],\n",
    "        'feature_scores_mda': [],\n",
    "        'logit_score_orig_onevar_list': [],\n",
    "        'logit_score_altered_onevar_list': [],\n",
    "    }\n",
    "for dset_num in tqdm(range(results.shape[0])): #tqdm(range(2)): #range(r.shape[0]):\n",
    "    row = r.iloc[dset_num]    \n",
    "\n",
    "    dset_name = row.dset_name # results['dset_names'][idx_0] #dsets.classification_dataset_names[0]\n",
    "    X, y = dsets.fetch_data(dset_name, return_X_y=True, \n",
    "                      local_cache_dir=data_dir)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=random_state)\n",
    "    num_features = X.shape[1]\n",
    "    rf = row.rf\n",
    "    assert(rf.score(test_X, test_y) == row.rf_test_score) # check that acc matches\n",
    "\n",
    "    feature_scores_mdi = scores.get_importance_scores(rf, score_type='mdi', X=test_X, Y=test_y)\n",
    "    feature_scores_mda = scores.get_importance_scores(rf, score_type='mda', X=test_X, Y=test_y)\n",
    "#     feature_ranks = np.argsort(feature_scores)\n",
    "#     print(f'feature_scores {feature_scores}\\nfeature_ranks {feature_ranks}')\n",
    "\n",
    "    score_results['feature_scores_mdi'].append(feature_scores_mdi)\n",
    "    score_results['feature_scores_mda'].append(feature_scores_mda)\n",
    "\n",
    "\n",
    "    logit_score_orig_onevar_list = []\n",
    "    logit_score_altered_onevar_list = []\n",
    "    for i in range(num_features):\n",
    "        feat_nums = [i] # list of length 1 - longer lists not supported yet\n",
    "        feat_vals = get_feats(X, feat_nums)\n",
    "        feat_val_min = np.min(X)\n",
    "        feat_val_max = np.max(X)\n",
    "#         print(f'min {feat_val_min} max {feat_val_max}')\n",
    "\n",
    "        # appropriate variable to get importance for\n",
    "        S = np.zeros(num_features)\n",
    "        S[feat_nums[0]]= 1\n",
    "\n",
    "        x_axis, scores_on_spaced_line = single_var_grid_scores_and_plot(rf, train_X, train_y, S, (feat_val_min, feat_val_max), plot=False)\n",
    "        f = interpolate.interp1d(x_axis, scores_on_spaced_line, kind='nearest') # function to interpolate the scores\n",
    "\n",
    "\n",
    "        logit_score_orig_onevar, logit_score_altered_onevar = score_logistic_onevar(train_X, train_y, test_X, test_y, feat_nums, f)\n",
    "        logit_score_orig_onevar_list.append(logit_score_orig_onevar)\n",
    "        logit_score_altered_onevar_list.append(logit_score_altered_onevar)    \n",
    "\n",
    "    score_results['logit_score_orig_onevar_list'].append(logit_score_orig_onevar_list)\n",
    "    score_results['logit_score_altered_onevar_list'].append(logit_score_altered_onevar_list)\n",
    "    \n",
    "    \n",
    "    # saving\n",
    "    scores_df = pd.DataFrame(score_results)\n",
    "    full_results = pd.concat([results.iloc[list(range(dset_num + 1))], scores_df], axis=1)\n",
    "    pkl.dump(full_results, open(oj(out_dir, f'full_results_{dset_num}.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load scores results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results = pd.DataFrame(pkl.load(open(oj(out_dir, 'full_results_162.pkl'), 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = []\n",
    "orig_mean = []\n",
    "alt_mean = []\n",
    "orig_top = []\n",
    "alt_top = []\n",
    "orig_top_mda = []\n",
    "alt_top_mda = []\n",
    "for i in range(full_results.shape[0]):\n",
    "    row = full_results.iloc[i]\n",
    "    orig = arr(row.logit_score_orig_onevar_list)\n",
    "    alt = arr(row.logit_score_altered_onevar_list)\n",
    "    feat_ranks = np.argsort(row.feature_scores_mdi)\n",
    "    max_feat = feat_ranks[-1]\n",
    "    orig_top.append(row.logit_score_orig_onevar_list[max_feat])\n",
    "    alt_top.append(row.logit_score_altered_onevar_list[max_feat])\n",
    "\n",
    "    feat_ranks = np.argsort(row.feature_scores_mda)\n",
    "    max_feat = feat_ranks[-1]\n",
    "    orig_top_mda.append(row.logit_score_orig_onevar_list[max_feat])\n",
    "    alt_top_mda.append(row.logit_score_altered_onevar_list[max_feat])    \n",
    "    \n",
    "#     plt.hist(alt - orig)\n",
    "    mean_scores.append(np.mean(alt - orig))\n",
    "    orig_mean.append(np.mean(orig))\n",
    "    alt_mean.append(np.mean(alt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, C, = 1, 4\n",
    "\n",
    "plt.figure(figsize=(C * 3, R * 3), dpi=300) #, dpi=300)\n",
    "\n",
    "plt.subplot(R, C, 1)\n",
    "plt.title('mean over feats')\n",
    "plt.plot(orig_mean, alt_mean, '.')\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.xlabel('orig acc')\n",
    "plt.ylabel('altered acc')\n",
    "\n",
    "plt.subplot(R, C, 2)\n",
    "plt.title('using max mdi feat')\n",
    "plt.plot(orig_top, alt_top, '.')\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.xlabel('orig acc')\n",
    "plt.ylabel('altered acc')\n",
    "\n",
    "plt.subplot(R, C, 3)\n",
    "plt.title('using max mda feat')\n",
    "plt.plot(orig_top_mda, alt_top_mda, '.')\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.xlabel('orig acc')\n",
    "plt.ylabel('altered acc')\n",
    "\n",
    "plt.subplot(R, C, 4)\n",
    "plt.title('compare mdi and mda')\n",
    "plt.plot(orig_top, orig_top_mda, '.')\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.xlabel('select w/ mdi')\n",
    "plt.ylabel('select w/ mda')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
